{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659ca9c0-1080-4c81-a143-2d6401b0bdd1",
   "metadata": {},
   "source": [
    "## Phase 1 - Setup & Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57019f4-93ce-4189-97e7-1020917addf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dependencies\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1e7c62-b12c-48fa-9c3d-c9db365b9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure\n",
    "os.makedirs(\"data/raw\", exist_ok=True)   # raw resumes (PDF/DOCX)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)  # cleaned extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbfbb11-6d75-43b7-a56a-dda18ddef1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from pdf\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b0fce7-fa8c-4231-b99e-b30f6e10fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from docx\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a DOCX file using python-docx.\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c02fa5-da8b-4e05-b9b4-2001c0f9ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle multiple file types\n",
    "def extract_resume_text(file_path: str) -> str:\n",
    "    \"\"\"Extract text depending on file type (PDF/DOCX).\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9590415-c2ac-47a6-91aa-7442ca9bf650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_018.pdf</td>\n",
       "      <td>EVELYNN ADAMS\\nGRADUATE FRESHER\\nPROFESSIONAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901841_RESUME.pdf</td>\n",
       "      <td>ANUVA GOYAL \\n \\nD.O.B.: 1st October 2000 \\nGe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AnuvaGoyal_Latex.pdf</td>\n",
       "      <td>ANUVA GOYAL\\n[ anuvagoyal111@gmail.com\\n½ Agra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_056.pdf</td>\n",
       "      <td>Christian Von\\nKelin\\nJ U N I O R  A N A L Y S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate_042.pdf</td>\n",
       "      <td>Ryan Nelson\\nF R E S H E R  S O F T W A R E  D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename                                               text\n",
       "0     candidate_018.pdf  EVELYNN ADAMS\\nGRADUATE FRESHER\\nPROFESSIONAL ...\n",
       "1    1901841_RESUME.pdf  ANUVA GOYAL \\n \\nD.O.B.: 1st October 2000 \\nGe...\n",
       "2  AnuvaGoyal_Latex.pdf  ANUVA GOYAL\\n[ anuvagoyal111@gmail.com\\n½ Agra...\n",
       "3     candidate_056.pdf  Christian Von\\nKelin\\nJ U N I O R  A N A L Y S...\n",
       "4     candidate_042.pdf  Ryan Nelson\\nF R E S H E R  S O F T W A R E  D..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process multiple files\n",
    "\n",
    "# Example: Process sample resumes in data/raw directory\n",
    "resume_dir = \"data/raw\"\n",
    "\n",
    "extracted_data = []\n",
    "\n",
    "for filename in os.listdir(resume_dir):\n",
    "    file_path = os.path.join(resume_dir, filename)\n",
    "    try:\n",
    "        text = extract_resume_text(file_path)\n",
    "        extracted_data.append({\"filename\": filename, \"text\": text})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "df_resumes = pd.DataFrame(extracted_data)\n",
    "df_resumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018017e9-c296-4d84-b825-526be88486f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed resumes saved at data/processed/resumes.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned text\n",
    "\n",
    "# Save processed results\n",
    "output_path = \"data/processed/resumes.csv\"\n",
    "df_resumes.to_csv(output_path, index=False)\n",
    "print(f\"✅ Processed resumes saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e4a0f-c37f-4360-8e10-d01da490d6a8",
   "metadata": {},
   "source": [
    "## Phase 2 - Information Extraction (NLP - Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c005c4-b896-4c2b-acd8-98d29480bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model \n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be3d2ac-ef09-401b-9534-8019d6996d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract emails and phone numbers with regex\n",
    "\n",
    "def extract_email(text: str):\n",
    "    \"\"\"Extract the first email found in the text.\"\"\"\n",
    "    match = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "\n",
    "def extract_phone(text: str):\n",
    "    \"\"\"Extract the first phone number (very general pattern).\"\"\"\n",
    "    match = re.search(r\"(\\+?\\d{1,3})?[\\s\\-]?\\(?\\d{2,4}\\)?[\\s\\-]?\\d{3,4}[\\s\\-]?\\d{3,4}\", text)\n",
    "    return match.group(0) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f69bc4-bca4-4850-8f53-43aff7b4e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidate name with spaCY NER\n",
    "\n",
    "def extract_name(text: str):\n",
    "    \"\"\"Extract a candidate's name using spaCy NER (PERSON entity).\"\"\"\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5ad136-e0a3-478a-a873-2af3725b312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract skills with dictionary lookup and semantic approach\n",
    "\n",
    "# Example skills dictionary \n",
    "skills_list = [\n",
    "    \"Python\", \"Java\", \"C++\", \"SQL\", \"Machine Learning\", \"Deep Learning\",\n",
    "    \"NLP\", \"Computer Vision\", \"TensorFlow\", \"PyTorch\",\n",
    "    \"React\", \"Node.js\", \"AWS\", \"Docker\", \"Kubernetes\"\n",
    "]\n",
    "\n",
    "def extract_skills(text: str, skills=skills_list):\n",
    "    \"\"\"Extract matching skills from text (case-insensitive).\"\"\"\n",
    "    found_skills = []\n",
    "    for skill in skills:\n",
    "        pattern = r\"\\b\" + re.escape(skill) + r\"\\b\"\n",
    "        if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "            found_skills.append(skill)\n",
    "    return list(set(found_skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187e5324-8a2f-49f0-93bd-26a53bed7f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>skills</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Machine Learning, Deep Learning, Python, Java...</td>\n",
       "      <td>candidate_018.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANUVA GOYAL</td>\n",
       "      <td>anuvagoyal111@gmail.com</td>\n",
       "      <td>+91 9520349542</td>\n",
       "      <td>[Computer Vision, SQL, Machine Learning, NLP, ...</td>\n",
       "      <td>1901841_RESUME.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>anuvagoyal111@gmail.com</td>\n",
       "      <td>None</td>\n",
       "      <td>[Computer Vision, SQL, Machine Learning, NLP, ...</td>\n",
       "      <td>AnuvaGoyal_Latex.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christian Von</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Python, Machine Learning]</td>\n",
       "      <td>candidate_056.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ryan Nelson</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Python, Java, Machine Learning]</td>\n",
       "      <td>candidate_042.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                    email           phone  \\\n",
       "0           None                     None            None   \n",
       "1    ANUVA GOYAL  anuvagoyal111@gmail.com  +91 9520349542   \n",
       "2           None  anuvagoyal111@gmail.com            None   \n",
       "3  Christian Von                     None            None   \n",
       "4    Ryan Nelson                     None            None   \n",
       "\n",
       "                                              skills              filename  \n",
       "0  [Machine Learning, Deep Learning, Python, Java...     candidate_018.pdf  \n",
       "1  [Computer Vision, SQL, Machine Learning, NLP, ...    1901841_RESUME.pdf  \n",
       "2  [Computer Vision, SQL, Machine Learning, NLP, ...  AnuvaGoyal_Latex.pdf  \n",
       "3                         [Python, Machine Learning]     candidate_056.pdf  \n",
       "4                   [Python, Java, Machine Learning]     candidate_042.pdf  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply extraction pipeline to resumes\n",
    "\n",
    "def parse_resume(text: str):\n",
    "    \"\"\"Run the full information extraction pipeline on resume text.\"\"\"\n",
    "    return {\n",
    "        \"name\": extract_name(text),\n",
    "        \"email\": extract_email(text),\n",
    "        \"phone\": extract_phone(text),\n",
    "        \"skills\": extract_skills(text)\n",
    "    }\n",
    "\n",
    "# Apply to our processed dataframe\n",
    "parsed_data = []\n",
    "\n",
    "for _, row in df_resumes.iterrows():\n",
    "    details = parse_resume(row[\"text\"])\n",
    "    details[\"filename\"] = row[\"filename\"]\n",
    "    parsed_data.append(details)\n",
    "\n",
    "df_parsed = pd.DataFrame(parsed_data)\n",
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560aac07-7991-4304-b649-c683c7e951f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsed resume info saved at data/processed/resumes_parsed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save parsed resume information\n",
    "\n",
    "output_parsed_path = \"data/processed/resumes_parsed.csv\"\n",
    "df_parsed.to_csv(output_parsed_path, index=False)\n",
    "print(f\"✅ Parsed resume info saved at {output_parsed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97686e6e-a3cb-4f2b-a8a2-46a7ec02007d",
   "metadata": {},
   "source": [
    "## Phase 3 - Embeddings and Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56b2116-d826-4d2c-88d2-cb1e2d4c3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b47dd4-0cb2-46d2-8acd-1f1571e68c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m             parts.append(\u001b[33m\"\u001b[39m\u001b[33mSkills: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(skills))\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(parts)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mlen\u001b[39m(df), df.columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/env-new/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mmake_document\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.notna(row.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m)): parts.append(\u001b[38;5;28mstr\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     11\u001b[39m parts.append(row[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m2000\u001b[39m])   \u001b[38;5;66;03m# truncate to reasonable length for speed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.notna(row.get(\u001b[33m\"\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# skills might be list-like or string; ensure string\u001b[39;00m\n\u001b[32m     14\u001b[39m     skills = row[\u001b[33m\"\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skills, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# 1) Prepare data: combine raw text + any important parsed fields into a single \"document\" to embed\n",
    "# Merging the two files: df_resumes has columns ['filename','text'] \n",
    "# and df_parsed has ['filename','name','email','phone','skills']\n",
    "\n",
    "# Merge on filename (if not already merged)\n",
    "df = df_resumes.merge(df_parsed, on=\"filename\", how=\"left\")\n",
    "\n",
    "# Create a single field to embed (you can tweak how much metadata to include)\n",
    "def make_document(row):\n",
    "    parts = []\n",
    "    if pd.notna(row.get(\"name\")): parts.append(str(row[\"name\"]))\n",
    "    parts.append(row[\"text\"][:2000])   # truncate to reasonable length for speed\n",
    "    if pd.notna(row.get(\"skills\")):\n",
    "        # skills might be list-like or string; ensure string\n",
    "        skills = row[\"skills\"]\n",
    "        if isinstance(skills, (list, tuple)):\n",
    "            parts.append(\"Skills: \" + \", \".join(skills))\n",
    "        else:\n",
    "            parts.append(\"Skills: \" + str(skills))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df[\"document\"] = df.apply(make_document, axis=1)\n",
    "len(df), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243c850-2eb7-476d-b77d-dabcddf10109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load sentence-transformers model\n",
    "# I recommend \"all-MiniLM-L6-v2\" for good speed/quality trade-off\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198ccca-333e-4508-a066-94ee3aaff468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create embeddings (batching for speed)\n",
    "batch_size = 64\n",
    "documents = df[\"document\"].tolist()\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)   # shape: (N, D)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15ea7b-e9d5-484a-a614-b502a9fdc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Build FAISS index (cosine similarity via normalized vectors -> inner product)\n",
    "d = embeddings.shape[1]   # embedding dimension\n",
    "index = faiss.IndexFlatIP(d)   # inner-product index (works with normalized vectors)\n",
    "index.add(embeddings)         # add vectors\n",
    "\n",
    "print(\"FAISS index: n_vectors =\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1476b-182d-4bab-bb0d-1da496e3eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Save index and metadata for persistence\n",
    "os.makedirs(\"models/faiss\", exist_ok=True)\n",
    "faiss.write_index(index, \"models/faiss/resumes_index.faiss\")\n",
    "np.save(\"models/faiss/resume_embeddings.npy\", embeddings)\n",
    "\n",
    "# Save metadata (filenames + parsed fields)\n",
    "df_meta = df[[\"filename\", \"name\", \"email\", \"phone\", \"skills\", \"document\"]].copy()\n",
    "df_meta.to_pickle(\"models/faiss/resume_metadata.pkl\")\n",
    "print(\"Saved FAISS index and metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530082d6-9dc3-4bf8-9033-342c54cfce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Search function: embed query, query FAISS, return ranked results\n",
    "def search_resumes(query: str, top_k: int = 5):\n",
    "    # 1) embed query (and normalize)\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # 2) search\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    scores = scores[0]      # shape (top_k,)\n",
    "    idxs = idxs[0]          # shape (top_k,)\n",
    "    results = []\n",
    "    \n",
    "    for score, i in zip(scores, idxs):\n",
    "        if i == -1:\n",
    "            continue\n",
    "        row = df_meta.iloc[i]\n",
    "        results.append({\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"name\": row[\"name\"],\n",
    "            \"email\": row[\"email\"],\n",
    "            \"phone\": row[\"phone\"],\n",
    "            \"skills\": row[\"skills\"],\n",
    "            \"score\": float(score),\n",
    "            \"snippet\": row[\"document\"][:500]   # short preview\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230a2d1-58c8-4aba-adc8-f24c451f9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Example queries\n",
    "queries = [\n",
    "    \"machine learning engineer with PyTorch and NLP experience\",\n",
    "    \"frontend engineer with React and Node.js\",\n",
    "    \"data scientist with SQL and deep learning\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\nQuery:\", q)\n",
    "    res = search_resumes(q, top_k=3)\n",
    "    for i, r in enumerate(res, 1):\n",
    "        print(f\"{i}. {r['filename']} | name: {r['name']} | score: {r['score']:.4f}\")\n",
    "        print(\"   skills:\", r[\"skills\"])\n",
    "        print(\"   snippet:\", r[\"snippet\"][:200].replace(\"\\n\",\" \") + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eede3-535a-4f53-aaa3-bff1f71617a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Add new resume(s) incrementally (without rebuilding entire index)\n",
    "\n",
    "# - Extract text -> build document -> embed -> add to index -> append to df_meta and save embeddings\n",
    "\n",
    "def add_new_resume(file_path: str, filename: str=None):\n",
    "    # extract text using your earlier function\n",
    "    text = extract_resume_text(file_path)   # from Phase 1\n",
    "    \n",
    "    # parse details using parse_resume (from Phase 2)\n",
    "    parsed = parse_resume(text)\n",
    "    \n",
    "    # build document\n",
    "    row = {\n",
    "        \"filename\": filename or os.path.basename(file_path),\n",
    "        \"name\": parsed.get(\"name\"),\n",
    "        \"email\": parsed.get(\"email\"),\n",
    "        \"phone\": parsed.get(\"phone\"),\n",
    "        \"skills\": parsed.get(\"skills\"),\n",
    "        \"document\": make_document({\"name\": parsed.get(\"name\"), \"text\": text, \"skills\": parsed.get(\"skills\")})\n",
    "    }\n",
    "    \n",
    "    # embed\n",
    "    emb = model.encode([row[\"document\"]], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # add to index\n",
    "    index.add(emb)\n",
    "    \n",
    "    # append to metadata dataframe\n",
    "    global df_meta\n",
    "    df_meta = pd.concat([df_meta, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # optionally save updated index + metadata\n",
    "    faiss.write_index(index, \"models/faiss/resumes_index.faiss\")\n",
    "    np.save(\"models/faiss/resume_embeddings.npy\", np.vstack([embeddings, emb]))\n",
    "    df_meta.to_pickle(\"models/faiss/resume_metadata.pkl\")\n",
    "    print(f\"Added {row['filename']} to index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481958c1-edce-4d59-8773-4b0c9d8f851d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env-new]",
   "language": "python",
   "name": "conda-env-env-new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
