{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269004a-565a-4575-a914-da13de98cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill Search and Matching (with skill expansion)\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# -----------------------------\n",
    "# Load the dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"resumes_cleaned.csv\")  # cleaned dataset from Phase 3\n",
    "\n",
    "# -----------------------------\n",
    "# Load embeddings model\n",
    "# -----------------------------\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# -----------------------------\n",
    "# Create resume embeddings\n",
    "# -----------------------------\n",
    "batch_size = 64\n",
    "documents = df[\"document\"].tolist()\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    emb = model.encode(batch, show_progress_bar=False,\n",
    "                       convert_to_numpy=True, normalize_embeddings=True)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# -----------------------------\n",
    "# Build FAISS index\n",
    "# -----------------------------\n",
    "d = embeddings.shape[1]  # embedding dimension\n",
    "index = faiss.IndexFlatIP(d)  # cosine similarity (with normalized vectors)\n",
    "index.add(embeddings)\n",
    "\n",
    "# -----------------------------\n",
    "# Curated Skills Taxonomy\n",
    "# -----------------------------\n",
    "curated_skills = [\n",
    "    \"Python\", \"Machine Learning\", \"Deep Learning\", \"NLP\", \"Computer Vision\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"Scikit-learn\", \"Data Analysis\", \"SQL\",\n",
    "    \"Spark\", \"Hadoop\", \"Data Engineering\", \"MLOps\", \"Docker\", \"Kubernetes\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Expand query using fuzzy matching + embeddings\n",
    "# -----------------------------\n",
    "def expand_skills(query, top_n=3, similarity_threshold=70):\n",
    "    \"\"\"\n",
    "    Expand a query skill with related terms from curated taxonomy.\n",
    "    Uses both fuzzy string matching and semantic similarity.\n",
    "    \"\"\"\n",
    "    expanded = set([query])\n",
    "\n",
    "    # --- Fuzzy matching (catch typos, close matches) ---\n",
    "    matches = process.extract(query, curated_skills, scorer=fuzz.WRatio, limit=top_n)\n",
    "    for match, score, _ in matches:\n",
    "        if score >= similarity_threshold:\n",
    "            expanded.add(match)\n",
    "\n",
    "    # --- Embedding similarity (semantic expansion) ---\n",
    "    query_vec = model.encode([query], normalize_embeddings=True)\n",
    "    skill_vecs = model.encode(curated_skills, normalize_embeddings=True)\n",
    "    sims = np.dot(skill_vecs, query_vec.T).flatten()\n",
    "\n",
    "    best_idx = np.argsort(sims)[::-1][:top_n]\n",
    "    for idx in best_idx:\n",
    "        expanded.add(curated_skills[idx])\n",
    "\n",
    "    return list(expanded)\n",
    "\n",
    "# -----------------------------\n",
    "# Search function with expansion\n",
    "# -----------------------------\n",
    "def search_resumes(query, k=5):\n",
    "    expanded_skills = expand_skills(query)\n",
    "    print(f\"Expanded skills for '{query}': {expanded_skills}\")\n",
    "\n",
    "    # Encode expanded query terms\n",
    "    query_vecs = model.encode(expanded_skills, normalize_embeddings=True)\n",
    "    \n",
    "    # Search for each expanded term and merge results\n",
    "    scores, indices = index.search(query_vecs, k)\n",
    "\n",
    "    # Collect top results\n",
    "    results = {}\n",
    "    for term, term_scores, term_idx in zip(expanded_skills, scores, indices):\n",
    "        for score, idx in zip(term_scores, term_idx):\n",
    "            if idx == -1: \n",
    "                continue\n",
    "            if idx not in results or score > results[idx][\"score\"]:\n",
    "                results[idx] = {\"doc\": df.iloc[idx][\"document\"], \"score\": float(score)}\n",
    "\n",
    "    # Sort by best score\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1][\"score\"], reverse=True)\n",
    "    return sorted_results[:k]\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "query = \"PyTorch\"\n",
    "results = search_resumes(query, k=5)\n",
    "\n",
    "for rank, (idx, item) in enumerate(results, start=1):\n",
    "    print(f\"\\nRank {rank} | Score: {item['score']:.4f}\")\n",
    "    print(item[\"doc\"][:300], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
